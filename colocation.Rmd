---
title: "Colocation data"
output:
  html_document:
    toc: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::knit_hooks$set(
  margin1 = function(before, options, envir) {
    if (before) par(mgp = c(1.5, .5, 0), bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  },
  margin2 = function(before, options, envir) {
    if (before) par(mgp = c(2, .5, 0), bty = "n", plt = c(.105, .97, .13, .97))
    else NULL
  }
)

knitr::opts_chunk$set(echo       = TRUE,
                      cache      = TRUE,
                      margin1     = TRUE,
                      fig.retina = 2,
                      fig.align  = "center")

l <- "en_US.UTF-8"
Sys.setenv(LANGAGE = l)
Sys.setlocale(locale = l)
Sys.setlocale("LC_MESSAGES", l)
```

## Packages

```{r message = FALSE}
library(sf)
library(stars)
library(osmdata)
library(magrittr)
library(stringr)
library(lubridate)
library(tidyr)
library(purrr)
library(dplyr) # best to load last
```

```{r}
hist2 <- function(...) hist(..., main = NA)
```

## Population density raster data

Downloading the population density data from [WorldPop](https://www.worldpop.org):

```{r eval = FALSE}
download.file("ftp://ftp.worldpop.org.uk/GIS/Population/Individual_countries/VNM/Viet_Nam_100m_Population/VNM_ppp_v2b_2020_UNadj.tif",
              "VNM_ppp_v2b_2020_UNadj.tif")
```

Loading the data:

```{r}
worldpop <- read_stars("VNM_ppp_v2b_2020_UNadj.tif")
```

## Google and Apple mobility data

Downloading the population mobility data:

```{r eval = FALSE}
download.file("https://www.dropbox.com/s/6fl62gcuma9890f/google.rds?raw=1", "google.rds")
download.file("https://www.dropbox.com/s/uuxxjm3cgs0a4gw/apple.rds?raw=1", "apple.rds")
```

Loading the data:

```{r eval = FALSE}
google <- readRDS("google.rds")
apple <- readRDS("apple.rds") %>% 
  mutate_if(is.numeric, subtract, 100)
```

## Polygons from GADM

Downloading the polygons from [GADM](https://gadm.org):

```{r eval = FALSE}
download.file("https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_VNM_0_sf.rds", "gadm36_VNM_0_sf.rds")
download.file("https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_VNM_1_sf.rds", "gadm36_VNM_1_sf.rds")
download.file("https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_VNM_2_sf.rds", "gadm36_VNM_2_sf.rds")
download.file("https://biogeo.ucdavis.edu/data/gadm3.6/Rsf/gadm36_VNM_3_sf.rds", "gadm36_VNM_3_sf.rds")
download.file("https://biogeo.ucdavis.edu/data/gadm2.8/rds/VNM_adm2.rds"       , "VNM_adm2.rds")
download.file("https://biogeo.ucdavis.edu/data/gadm2.8/rds/VNM_adm3.rds"       , "VNM_adm3.rds")
```

Loading the polygons:

```{r}
vn0 <- readRDS("gadm36_VNM_0_sf.rds")     # country polygon
vn1 <- readRDS("gadm36_VNM_1_sf.rds")     # provinces polygons

vn2 <- readRDS("gadm36_VNM_2_sf.rds") %>% # districts polygons
  transmute(province = str_squish(NAME_1),
            district = str_squish(NAME_2) %>%
              str_remove_all("Thành Phố |Thị Xã |Quận "))

vn3 <- readRDS("gadm36_VNM_3_sf.rds") %>% # districts polygons
  transmute(province = str_squish(NAME_1),
            district = str_squish(NAME_2) %>%
              str_remove_all("Thành Phố |Thị Xã |Quận "),
            commune  = str_squish(NAME_3))

vn2_old <- readRDS("VNM_adm2.rds") %>%    # old version of the districts polygons
  st_as_sf() %>% 
  transmute(province = str_squish(NAME_1),
            district = str_squish(NAME_2) %>% 
              str_remove_all("Thành Phố |Thị Xã |Quận ") %>% 
              str_replace("Chiêm Hoá", "Chiêm Hóa"))

vn3_old <- readRDS("VNM_adm3.rds") %>%    # old version of the communes polygons
  st_as_sf() %>% 
  transmute(province = str_squish(NAME_1),
            district = str_squish(NAME_2) %>% 
              str_remove_all("Thành Phố |Thị Xã |Quận ") %>% 
              str_replace("Chiêm Hoá", "Chiêm Hóa"),
            commune = str_squish(NAME_3)) %>% 
  arrange(province, district, commune)
```

Removing the commune Huæi Luông from the district Sìn Hồ:

```{r}
vn2_old %<>% 
  filter(district == "Sìn Hồ") %>% 
  st_set_geometry(st_union(filter(vn3_old, district == "Sìn Hồ", commune != "Huæi Luông"))) %>% 
  rbind(filter(vn2_old, district != "Sìn Hồ")) %>% 
  arrange(province, district)
```

## Adding the polygon for Côn Đảo from OpenStreetMap

Downloading the administrative boundaries from [OpenStreetMap](https://www.openstreetmap.org):

```{r}
con_dao <- c(106.523384, 8.622214, 106.748218, 8.782639) %>%
  opq() %>% 
  add_osm_feature(key = "boundary", value = "administrative") %>% 
  osmdata_sf()
```

The main island is made of lines. Let's transform them into a polygon:

```{r}
main_island <- con_dao$osm_lines %>%
  st_combine() %>%
  st_polygonize() %>%
  st_geometry() %>%
  st_collection_extract("POLYGON")
```

The other islands are already polygons. Let's extract and merge them with the
newly created polygon of the main island:

```{r}
con_dao <- con_dao$osm_polygons %>%
  st_geometry() %>% 
  c(main_island) %>% 
  st_multipolygon() %>% 
  st_sfc(crs = st_crs(vn2))
```

Let's add the polygon of Côn Đảo to the GADM data:

```{r}
vn2 %<>%
  filter(province == "Bà Rịa - Vũng Tàu") %>% 
  head(1) %>% 
  mutate(district = "Côn Đảo") %>% 
  st_set_geometry(con_dao) %>% 
  rbind(vn2)
```

## Adding the census 2019 data

For some reason the province of `Vĩnh Long` is missing... We add information
form [Wikipedia](https://en.wikipedia.org/wiki/Vĩnh_Long_Province#Administrative_divisions):

```{r}
census <- readRDS("~/Dropbox/aaa/studies/oucomo/clean data/census2019.rds") %>% 
  group_by(province, district) %>% 
  summarise(n = sum(n)) %>% 
  ungroup() %>% 
  mutate(province = str_squish(province) %>%
                    str_remove_all("Thành phố |Tỉnh ") %>% 
                    str_replace("Đăk Lăk"             , "Đắk Lắk") %>% 
                    str_replace("Đăk Nông"            , "Đắk Nông") %>% 
                    str_replace("Khánh Hoà"           , "Khánh Hòa") %>% 
                    str_replace("Thanh Hoá"           , "Thanh Hóa"),
         district = str_squish(district) %>%
                    str_replace("Thành phố Cao Lãnh"  , "Cao Lãnh (Thành phố)") %>% 
                    str_replace("Thị xã Hồng Ngự"     , "Hồng Ngự (Thị xã)") %>% 
                    str_replace("Thị xã Kỳ Anh"       , "Kỳ Anh (Thị xã)") %>% 
                    str_replace("Thị xã Long Mỹ"      , "Long Mỹ (Thị xã)") %>% 
                    str_replace("Thị xã Cai Lậy"      , "Cai Lậy (Thị xã)") %>% 
                    str_replace("Thị xã Duyên Hải"    , "Duyên Hải (Thị xã)") %>% 
                    str_remove_all("Huyện |Huỵên |Quận |Thành phố |Thành Phô |Thành Phố |Thị xã |Thị Xã ") %>% 
                    str_replace("Hoà Vang"            , "Hòa Vang") %>% 
                    str_replace("Ứng Hoà"             , "Ứng Hòa") %>% 
                    str_replace("Đồng Phù"            , "Đồng Phú") %>% 
                    str_replace("Đắk Glong"           , "Đăk Glong") %>% 
                    str_replace("Ia H’Drai"           , "Ia H' Drai") %>% 
                    str_replace("ý Yên"               , "Ý Yên") %>% 
                    str_replace("Bác ái"              , "Bác Ái") %>% 
                    str_replace("Phan Rang- Tháp Chàm", "Phan Rang-Tháp Chàm") %>% 
                    str_replace("Đông Hoà"            , "Đông Hòa") %>% 
                    str_replace("Tuy Hòa"             , "Tuy Hoà") %>% 
                    str_replace("Thiệu Hoá"           , "Thiệu Hóa")) %>% 
  bind_rows(
    bind_cols(
      data.frame(province = rep("Vĩnh Long", 8)),
      tribble(
        ~district  ,     ~n,
        "Bình Tân" ,  93758, # 2009
        "Long Hồ"  , 160537, # 2018
        "Mang Thít", 103573, # 2018
        "Tam Bình" , 162191, # 2003
        "Trà Ôn"   , 149983, # 2003
        "Vũng Liêm", 176233, # 2003
        "Bình Minh",  95282, # 2003
        "Vĩnh Long", 200120  # 2018
      )
    ),
    tribble(
      ~province  , ~district     , ~n, 
      "Điện Biên", "Mường Ảng"   ,  37077, # 2006
      "Hải Phòng", "Bạch Long Vĩ",    912, # 2018
      "Phú Thọ"  , "Thanh Sơn"   , 187700, # 2003
      "Quảng Trị", "Cồn Cỏ"      ,    400  # 2003
    )
  )
```

```{r}
identical(sort(unique(census$province)), sort(unique(vn2$province)))
```

```{r}
anti_join(census, vn2, c("province", "district"))
anti_join(vn2, census, c("province", "district"))
```

```{r}
vn2 %<>% left_join(census, c("province", "district"))
```

```{r}
vn2 %>% 
  st_drop_geometry() %>% 
  group_by(province, district) %>% 
  tally() %>% 
  filter(n > 1)
```

## Merging some districts

The colocation data use the [Bing](https://www.bing.com/maps) polygons, which do
not seem to be very much up to date. In order to adjust for that, we need to
merge the following districts in the GADM data:

* Hà Tĩnh: merge Kỳ Anh (Thị xã) into Kỳ Anh that split in 2015
* Hậu Giang: merge Long Mỹ (Thị xã) into Long Mỹ that split in 2015
* Tiền Giang: merge Cai Lậy (Thị xã) into Cai Lậy that split in 2013
* Trà Vinh: merge Duyên Hải (Thị xã) into Duyên Hải that split in 2015
* Bình Dương: merge Bắc Tân Uyên into Tân Uyên that split in 2013
* Bình Dương: merge Bàu Bàng into Bến Cát that split in 2013
* Hà Nội: merge Bắc Từ Liêm and Nam Từ Liêm into Từ Liêm that split in 2013
* Long An: merge Kiến Tường into Mộc Hóa that split in 2013
* Nghệ An: merge Hoàng Mai into Quỳnh Lưu that split in 2013
* Quảng Bình: merge Ba Đồn into Quảng Trạch that split in 2013

Furthermore, some of the districts need to be split in two, with each part
merged to a different district. That's the case for these 3 districts:

* Điện Biên: split Nậm Pồ into 2 parts and merge them with Mường Chà and Mường Nhé (2012)
* Lai Châu: split Nậm Nhùn into 2 parts and merge them with Mường Tè and Sìn Hồ (2012)
* Tuyên Quang: split Lâm Bình into 2 parts and merge them with Nà Hang and Chiêm Hóa (2011)

As illustrated below:

```{r}
plot_districts <- function(d1, d2, d3) {
  vn2 %>% 
    filter(district %in% c(d1, d2, d3)) %>% 
    st_geometry() %>% 
    plot(col = "grey")
  
  vn2 %>% 
    filter(district == d1) %>% 
    st_geometry() %>% 
    plot(add = TRUE, col = "red")
  
  vn2_old %>% 
    filter(district %in% c(d2, d3)) %>% 
    st_geometry() %>% 
    plot(add = TRUE, border = "blue", lwd = 2)
}
```

```{r}
plot_districts("Nậm Pồ"  , "Mường Chà", "Mường Nhé")
plot_districts("Lâm Bình", "Nà Hang"  , "Chiêm Hóa")
plot_districts("Nậm Nhùn", "Mường Tè" , "Sìn Hồ")
```

Here are two functions to fix these 2 above-mentioned problems. Here is the
first function:

```{r}
merge_districts <- function(vn, d1, d2, p) {
  dst <- c(d1, d2)
  
  tmp <- vn %>% 
    filter(district %in% dst, province == p) %>% 
    mutate(n = sum(n))
  
  tmp %<>% 
    filter(district %in% d1) %>% 
    st_set_geometry(st_union(tmp))
  
  vn %>% 
    filter(! (district %in% dst & province == p)) %>% 
    rbind(tmp) %>% 
    arrange(province, district)
}
```

The second function needs this function:

```{r}
proportion <- function(to_cut, one_district, new_vn = vn2, old_vn = vn2_old, rstr = worldpop) {
  to_cut <- filter(new_vn, district == to_cut)
  one_part <- st_intersection(to_cut, filter(old_vn, district == one_district))
  
  wp0 <- rstr %>%
    st_crop(to_cut) %>% 
    st_as_stars() %>% 
    unlist() %>% 
    sum(na.rm = TRUE)
  
  rstr %>% 
    st_crop(one_part) %>% 
    st_as_stars() %>% 
    unlist() %>% 
    sum(na.rm = TRUE) %>% 
    divide_by(wp0)
}
```

Let's try it:

```{r warning = FALSE, message = FALSE}
proportion("Nậm Nhùn", "Mường Tè" , vn2, vn2_old, worldpop)
proportion("Nậm Pồ"  , "Mường Chà", vn2, vn2_old, worldpop)
proportion("Lâm Bình", "Nà Hang"  , vn2, vn2_old, worldpop)
```

And here is the second function we needed:

```{r}
merge_back_districts <- function(c2, d1, d2, d3, c1 = vn2_old, rstr = worldpop) {
  dsts <- c(d1, d2, d3)
  
  tmp <- c2 %>% 
    filter(district %in% dsts) %$%
    setNames(n, district)

  half1 <- round(proportion(d1, d2, c2, c1, rstr) * tmp[d1])

  half2 <- tmp[d1] - half1
  tmp[d2] <- tmp[d2] + half1
  tmp[d3] <- tmp[d3] + half2
  tmp <- tmp[dsts[-1]]
  
  c1 %>% 
    filter(district %in% dsts[-1]) %>% 
    mutate(n = tmp) %>% 
    select(everything(), geometry) %>% 
    rbind(filter(c2, ! district %in% dsts)) %>% 
    arrange(province, district)
}
```

Let's now call these 2 functions to do the mergings:

```{r warning = FALSE, message = FALSE}
vn2 %<>%
  merge_districts("Kỳ Anh"     , "Kỳ Anh (Thị xã)"   , "Hà Tĩnh") %>% 
  merge_districts("Long Mỹ"    , "Long Mỹ (Thị xã)"  , "Hậu Giang") %>% 
  merge_districts("Cai Lậy"    , "Cai Lậy (Thị xã)"  , "Tiền Giang") %>% 
  merge_districts("Duyên Hải"  , "Duyên Hải (Thị xã)", "Trà Vinh") %>% 
  merge_districts("Tân Uyên"   , "Bắc Tân Uyên"      , "Bình Dương") %>%
  merge_districts("Bến Cát"    , "Bàu Bàng"          , "Bình Dương") %>% 
  merge_districts("Bắc Từ Liêm", "Nam Từ Liêm"       , "Hà Nội") %>% # then rename to Từ Liêm
  merge_districts("Mộc Hóa"    , "Kiến Tường"        , "Long An") %>% 
  merge_districts("Quỳnh Lưu"  , "Hoàng Mai"         , "Nghệ An") %>% 
  merge_districts("Quảng Trạch", "Ba Đồn"            , "Quảng Bình") %>% 
  merge_back_districts("Nậm Pồ"  , "Mường Chà", "Mường Nhé") %>% 
  merge_back_districts("Nậm Nhùn", "Mường Tè" , "Sìn Hồ") %>% 
  merge_back_districts("Lâm Bình", "Nà Hang"  , "Chiêm Hóa") %>% 
  mutate(district = str_replace(district, "Bắc Từ Liêm", "Từ Liêm")) # here the renaming
```

Let's calculate the areas:

```{r}
vn2 %<>% 
  mutate(area = vn2 %>%
           st_geometry() %>% 
           st_area()) %>% 
  select(everything(), geometry)
```

## Visualizations

### Population sizes

```{r}
hist2(vn2$n, n = 50, xlab = "population size", ylab = "number of districts", axes = FALSE)
axis(1, seq(0, 1e6, 2e5), format(seq(0, 10, 2) * 1e5, big.mark = ",", scientific = FALSE, trim = TRUE))
axis(2)
```

```{r}
cb <- RColorBrewer::brewer.pal(9, "YlOrBr")
color_generator <- colorRampPalette(cb)
pal <- color_generator(10)
```

```{r}
hist2(vn2$n, quantile(vn2$n, seq(0, 1, le = 11)), col = pal, axes = FALSE,
      xlab = "population size", ylab = "density of probability")
axis(1, seq(0, 1e6, 2e5), format(seq(0, 10, 2) * 1e5, big.mark = ",", scientific = FALSE, trim = TRUE))
axis(2)
```

```{r}
vn2 %>% 
  select(n) %>% 
  plot(lwd = .1, col = pal[cut(vn2$n, quantile(vn2$n, seq(0, 1, le = 11)))], main = NA)

vn0 %>% 
  st_geometry() %>% 
  plot(add = TRUE)
```

```{r}
mean(vn2$n)
median(vn2$n)
```

### Areas

```{r margin1 = FALSE, margin2 = TRUE}
hist2(vn2$area / 1e6, n = 50,
      xlab = expression(paste("area ", (km^2))), ylab = "number of districts")
```

```{r}
mean(as.numeric(vn2$area / 1e6))
median(as.numeric(vn2$area / 1e6))
```

### Densities

```{r}
(quants <- quantile(as.numeric(1e6 * vn2$n / vn2$area), c(.025, .25, .5, .75, .975)))
```

```{r margin1 = FALSE, margin2 = TRUE}
hist2(log10(as.numeric(1e6 * vn2$n / vn2$area)), n = 50, axes = FALSE,
      xlab = expression(paste("density (/", km^2, ")")), ylab = "number of districts")
axis(1, 1:4, c("10", "100", "1000", "10000"))
axis(2)
abline(v = log10(quants), lty = c(3, 2, 1, 2, 3), col = "blue")
```

```{r margin1 = FALSE, margin2 = TRUE}
xs <- log10(as.numeric(1e6 * vn2$n / vn2$area))
hist2(xs, quantile(xs, seq(0, 1, le = 11)), col = pal, axes = FALSE,
      xlab = expression(paste("density (/", km^2, ")")), ylab = "density of probability")
axis(1, 1:4, c("10", "100", "1000", "10000"))
axis(2)
```

```{r}
vn2 %>% 
  transmute(dens = 1e6 * n / area) %>% 
  plot(lwd = .1, col = pal[cut(vn2$n, quantile(vn2$n, seq(0, 1, le = 11)))], main = NA)

vn0 %>% 
  st_geometry() %>% 
  plot(add = TRUE)
```

## Colocation data

The colocation data files are in the following folder:

```{r}
folder <- "~/Dropbox/aaa/studies/data for good/data/geoinsights/colocation maps"
```

The list of files:

```{r}
files <- dir(folder)
```

There are `r length(files)` of them:

```{r}
length(files)
```

Making names from files names:

```{r}
weeks <- str_remove_all(files, "^.*__|.csv")
```

Loading the colocation data into a list (one slot per week):

```{r message = FALSE}
colocation <- paste0(folder, "/", dir(folder)) %>%
  map(readr::read_csv) %>%
  setNames(weeks)
```

The colocation data looks like this:

```{r}
head(colocation, 1)
```

Interesting to see, by the way, the effect of the lockdown:

```{r}
nd <- nrow(vn2)
ld <- ymd(c(20200401, 20200422))
ys <- c(-1e5, 6e5)
xs <- ymd(names(colocation)) - 6
nb_links <- map_dbl(colocation, nrow)
xlim <- ymd(c("2020-02-29", "2020-07-01"))
plot(xs, nb_links, type = "s", xlab = NA, ylab = "number of links",
     xlim = xlim, ylim = c(0, nd^2), col = "blue")
abline(h = nd^2, lty = 2, col = "grey")
polygon(c(ld, rev(ld)), rep(ys, each = 2), col = adjustcolor("red", .25), border = NA)
```

Let's look at the sum of the probability per week:

```{r}
link_val <- map(colocation, pull, link_value)
plot(xs, map_dbl(link_val, sum), type = "s", col = "blue", xlim = xlim,
     xlab = NA, ylab = "sums of probabilities")
polygon(c(ld, rev(ld)), rep(ys, each = 2), col = adjustcolor("red", .25), border = NA)
```

The 10th week looks suspicious. There is no missing value and no zeros in the
link variable:

```{r}
link_val %<>% unlist()
any(is.na(link_val))
range(link_val)
```

## Getting rid off whatever is not linked to Vietnam

### The problem

Here we show what the problem is (i.e. some of the data are outside Vietnam).
The following function plots the colocation data:

```{r}
plot_fb <- function(df, xlim, ylim, col) {
  plot(xlim, ylim, asp = 1, xlab = "longitude", ylab = "latitude", type = "n")
  maps::map(col = "grey", fill = TRUE, add = TRUE)
  points(df[[1]], df[[2]], pch = ".", col = col)
  axis(1)
  axis(2)
  box(bty = "o")
}
```

Let's consider one week:

```{r}
june23 <- colocation$`2020-06-23`
```

The locations in this week are within these boundaries:

```{r}
(xlim <- range(range(june23$lon_1), range(june23$lon_2)))
(ylim <- range(range(june23$lat_1), range(june23$lat_2)))
```

Let's plot the polygon 1:

```{r}
june23 %>%
  select(lon_1, lat_1) %>% 
  distinct() %>% 
  plot_fb(xlim, ylim, "blue")
```

And the polygon 2:

```{r}
june23 %>%
  select(lon_2, lat_2) %>% 
  distinct() %>% 
  plot_fb(xlim, ylim, "red")
```

### The solution

In the following function, `df` is a data frame with the same column names as
a "colocation map" data frame. `pl` is an `sf` non-projected polygon. `type` is
either `1` or `2`.

```{r}
pts_in_pol <- function(type, df, pl, project = FALSE) {
  # assumes that sf is not projected.
  # 4326: non-projected
  # 3857: pseudo-Mercator (e.g. Google Maps)
  df %<>% st_as_sf(coords = paste0(c("lon_", "lat_"), type), crs = 4326)
  if (project) {
    df %<>% st_transform(3857)
    pl %<>% st_transform(3857)
  }
  df %>%
    st_intersects(pl) %>% 
    map_int(length)
}
```

The function returns a vector of length equal to the number of rows of `df` with
`1` if the points is inside the polygon `pl` and `0` otherwise. Let's try it:

```{r}
tmp <- pts_in_pol(1, june23, vn0)
```

It takes 3.5' and it's about 3 times slower if we project the points and polygon.
The arguments of the following function are the same as the previous one. It
uses the previous one to delete the records from `df` that have start and end
coordinates that are outside `pl`:

```{r}
rcd_in_pol <- function(df, pl, project = FALSE) {
  require(magrittr)
  1:2 %>%
    parallel::mclapply(pts_in_pol, df, pl, project, mc.cores = 2) %>% 
    as.data.frame() %>%
    rowSums() %>% 
    is_greater_than(0) %>% 
    magrittr::extract(df, ., ) # there is an extract() function in tidyr too
}
```

Let's try it:

```{r eval = FALSE}
june23 %<>% rcd_in_pol(vn0)
```

Let's process all the data (takes about 1 minute):

```{r}
colocation %<>% map(rcd_in_pol, vn0)
```

## Working out a district ID common to GADM + census and colocation data

The list of district in the colocation data:

```{r}
col_names <- c("polygon_id", "polygon_name", "lon", "lat", "name_stack")

districts1 <- map_dfr(colocation, select, polygon1_id, polygon1_name, lon_1, lat_1, name_stack_1) %>% 
  setNames(col_names)

districts2 <- map_dfr(colocation, select, polygon2_id, polygon2_name, lon_2, lat_2, name_stack_2) %>% 
  setNames(col_names)

districts <- bind_rows(districts1, districts2) %>%
  distinct()
```

### Province name missing for some districts of Hanoi

In the colocation data, the `name_stack_*` variables contains the names of the
province and the district separated by ` // `. The problem is that there are a
number of districts that do not have ` // ` in their `name_stack` variable and
all of them seem to be in the province of Hanoi:

```{r}
plot(st_geometry(vn0), col = "grey")

vn1 %>%
  filter(NAME_1 == "Hà Nội") %>%
  st_geometry() %>%
  plot(add = TRUE, col = "yellow")

districts %>% 
  filter(! grepl(" // ", name_stack)) %>% 
  st_as_sf(coords = c("lon", "lat"), crs = 4326) %>% 
  st_geometry() %>% 
  plot(add = TRUE, col = "red")
```

### Separating province name from district name in the colocation data

Plus a number of names fixes:

```{r}
districts %<>% 
  separate(name_stack, c("province", "district"), " // ") %>% 
  mutate(indicate = is.na(district),
         district = ifelse(indicate, province, district),
         province = ifelse(indicate, "Hanoi City", province) %>% 
           str_squish() %>% 
           str_remove(" Province| City") %>% 
           str_replace("-", " - ") %>% 
           str_replace("Da Nang"     , "Đà Nẵng") %>%
           str_replace("Hanoi"       , "Hà Nội") %>% 
           str_replace("Hai Phong"   , "Hải Phòng") %>%
           str_replace("Ho Chi Minh" , "Hồ Chí Minh") %>%
           str_replace("Hòa Bình"    , "Hoà Bình"),
         polygon_name = str_squish(polygon_name) %>% 
           str_replace("Thành Phố Cao Lãnh", "Cao Lãnh (Thành phố)") %>% 
           str_replace("Thị Xã Hồng Ngự", "Hồng Ngự (Thị xã)") %>% 
           str_remove("Huyện |Thành phố |Thị xã |Quận |Thành Phố |Thị Xã ") %>%
           str_replace("Quy Nhơn"    , "Qui Nhơn") %>% 
           str_replace("Đảo Phú Quý" , "Phú Quí") %>% 
           str_replace("Bình Thủy"   , "Bình Thuỷ") %>% 
           str_replace("Hòa An"      , "Hoà An") %>% 
           str_replace("Phục Hòa"    , "Phục Hoà") %>% 
           str_replace("Thái Hòa"    , "Thái Hoà") %>% 
           str_replace("Hạ Hòa"      , "Hạ Hoà") %>% 
           str_replace("Phú Hòa"     , "Phú Hoà") %>% 
           str_replace("Tây Hòa"     , "Tây Hoà") %>% 
           str_replace("Tuy Hòa"     , "Tuy Hoà") %>% 
           str_replace("Krông Ana"   , "Krông A Na") %>% 
           str_replace("Krông A Na"  , "Krông A Na") %>% ##
           str_replace("Krông Păk"   , "Krông Pắc") %>% 
           str_replace("Krông Pắc"   , "Krông Pắc") %>% ##
           str_replace("Đắk Glong"   , "Đăk Glong") %>% 
           str_replace("Đắk Rlấp"    , "Đắk R'Lấp") %>% 
           str_replace("A Yun Pa"    , "Ayun Pa") %>% 
           str_replace("Từ Liêm"     , "Nam Từ Liêm") %>% 
           str_replace("Kiến Thụy"   , "Kiến Thuỵ") %>% 
           str_replace("Thủy Nguyên" , "Thuỷ Nguyên") %>% 
           str_replace("Vị Thủy"     , "Vị Thuỷ") %>% 
           str_replace("Bác Ai"      , "Bác Ái") %>% 
           str_replace("Thanh Thủy"  , "Thanh Thuỷ") %>% 
           str_replace("Yên Hưng"    , "Quảng Yên") %>% 
           str_replace("Na Hang"     , "Nà Hang") %>% 
           str_replace("Mù Cang Chải", "Mù Căng Chải") %>%
           str_replace("M`Đrắk"      , "M'Đrắk") %>% 
           str_replace("Cư M`Gar"    , "Cư M'gar") %>% 
           str_replace("Ea H`Leo"    , "Ea H'leo") %>% 
           str_replace("Nam Từ Liêm" , "Từ Liêm") %>% 
           str_replace("Buôn Hồ"     , "Buôn Hồ"),
         polygon_name = ifelse(province == "Bạc Liêu" & polygon_name == "Hòa Bình",
                               "Hoà Bình", polygon_name)) %>% 
  select(-indicate, -district) %>% 
  rename(district = polygon_name)
```

The warnings are produced when the provinces names are missing for some of the
districts of Hanoi. Separating the districts names per province for both sources:

```{r}
anti_join(districts, vn2, c("province", "district"))
anti_join(vn2, districts, c("province", "district"))
```

Let's map these districts that never have any data:

```{r}
vn0 %>% 
  st_geometry() %>% 
  plot(col = "grey")

vn2 %>% 
  mutate(pd = paste(province, district)) %>% 
  filter(! pd %in% with(districts, paste(province, district))) %>% 
  st_geometry() %>% 
  plot(add = TRUE, col = "red")
```

Let's add these 5 districts to `districts`:

```{r}
tmp <- vn2 %>% 
  mutate(pd = paste(province, district)) %>% 
  filter(! pd %in% with(districts, paste(province, district)))

districts <- tmp %>%
  st_centroid() %>% 
  st_coordinates() %>% 
  as_tibble() %>% 
  transmute(lon = X, lat = Y) %>% 
  bind_cols(tmp) %>% 
  mutate(polygon_id = 850001:850005) %>% 
  select(polygon_id, district, lon, lat, province) %>% 
  bind_rows(districts)
```

## Rearranging the colocation data into a matrix

Let's generate a template with all the combinations of districts:

```{r}
template <- districts %>% 
  arrange(polygon_id) %$%
  expand.grid(polygon_id, polygon_id) %>% 
  as_tibble() %>% 
  setNames(c("polygon1_id", "polygon2_id"))
```

The function that transform the colocation data into a matrix:

```{r}
to_matrix <- function(df, template) {
  dim_names <- sort(unique(template$polygon1_id))
  df %>% 
    select(polygon1_id, polygon2_id, link_value) %>% 
    left_join(template, ., c("polygon1_id", "polygon2_id")) %>%
    mutate(link_value = replace_na(link_value, 0)) %>% 
    pull(link_value) %>% 
    matrix(nrow(districts)) %>%
    `colnames<-`(dim_names) %>% 
    `rownames<-`(dim_names)
}
```

Let's do it for all the weeks and sum them:

```{r}
coloc_mat <- colocation %>% 
  map(to_matrix, template) %>% 
  reduce(`+`)
```


